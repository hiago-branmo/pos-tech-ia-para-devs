"""
C√≥digo para adicionar no notebook de modelagem - CROSS-VALIDATION
Inserir AP√ìS o treinamento dos 3 modelos e ANTES da se√ß√£o "9. Compara√ß√£o de Modelos"
"""

# ============================================================================
# C√âLULA MARKDOWN
# ============================================================================
"""
## 8.5. Valida√ß√£o Cruzada (Cross-Validation)

### O que √© Cross-Validation?

**Cross-Validation** √© uma t√©cnica de valida√ß√£o que divide os dados em m√∫ltiplas parti√ß√µes (folds) para:
- **Avaliar robustez** do modelo em diferentes subconjuntos de dados
- **Detectar overfitting** comparando performance entre folds
- **Obter estimativa mais confi√°vel** da performance real

### Por que √© importante?

‚úÖ **Usa melhor os dados** - Todos os exemplos s√£o usados para treino E valida√ß√£o
‚úÖ **Reduz vari√¢ncia** - M√∫ltiplas avalia√ß√µes = resultado mais est√°vel
‚úÖ **Detecta overfitting** - Se performance varia muito entre folds = instabilidade
‚úÖ **Requerido academicamente** - Boas pr√°ticas de ML

### Estrat√©gia: 5-Fold Stratified Cross-Validation

```
Fold 1: [TRAIN][TRAIN][TRAIN][TRAIN][TEST ]
Fold 2: [TRAIN][TRAIN][TRAIN][TEST ][TRAIN]
Fold 3: [TRAIN][TRAIN][TEST ][TRAIN][TRAIN]
Fold 4: [TRAIN][TEST ][TRAIN][TRAIN][TRAIN]
Fold 5: [TEST ][TRAIN][TRAIN][TRAIN][TRAIN]
```

**Stratified:** Mant√©m propor√ß√£o de classes em cada fold
"""

# ============================================================================
# C√âLULA DE C√ìDIGO
# ============================================================================

from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np

print("=" * 80)
print("VALIDA√á√ÉO CRUZADA (5-FOLD STRATIFIED CROSS-VALIDATION)")
print("=" * 80)

# Configurar Cross-Validation
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# M√©tricas a avaliar
scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

# Dicion√°rio para armazenar resultados
cv_results = {}

# Lista de modelos a avaliar
models_to_validate = [
    ('Logistic Regression', lr_model),
    ('Random Forest', rf_model),
    ('XGBoost', xgb_model)
]

print("\n‚è≥ Executando valida√ß√£o cruzada (pode levar alguns minutos)...\n")

# Avaliar cada modelo
for model_name, model in models_to_validate:
    print("-" * 80)
    print(f"Validando: {model_name}")
    print("-" * 80)

    cv_results[model_name] = {}

    for metric in scoring_metrics:
        # Executar cross-validation
        scores = cross_val_score(
            model, X_train, y_train,
            cv=cv_strategy,
            scoring=metric,
            n_jobs=-1
        )

        cv_results[model_name][metric] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'min': scores.min(),
            'max': scores.max()
        }

        # Exibir resultados
        print(f"\n{metric.upper()}:")
        print(f"  M√©dia: {scores.mean():.4f} (+/- {scores.std():.4f})")
        print(f"  Min: {scores.min():.4f} | Max: {scores.max():.4f}")
        print(f"  Scores por fold: {[f'{s:.4f}' for s in scores]}")

print("\n" + "=" * 80)
print("‚úì Valida√ß√£o cruzada conclu√≠da!")
print("=" * 80)


# ============================================================================
# C√âLULA MARKDOWN
# ============================================================================
"""
### Interpretando os Resultados de Cross-Validation

**M√©dia:** Performance m√©dia nos 5 folds (melhor estimativa da performance real)
**Desvio Padr√£o (¬±):** Variabilidade entre folds
- Baixo (< 0.02): Modelo est√°vel
- M√©dio (0.02-0.05): Variabilidade normal
- Alto (> 0.05): Modelo inst√°vel ou dados heterog√™neos

**Min/Max:** Performance pior e melhor casos
**Scores por fold:** Performance individual em cada parti√ß√£o
"""


# ============================================================================
# C√âLULA DE C√ìDIGO - Compara√ß√£o Visual
# ============================================================================

# Criar tabela comparativa
print("\n" + "=" * 80)
print("RESUMO COMPARATIVO - CROSS-VALIDATION")
print("=" * 80 + "\n")

cv_comparison = pd.DataFrame({
    'Modelo': [name for name in cv_results.keys()],
    'Accuracy (CV)': [cv_results[name]['accuracy']['mean'] for name in cv_results.keys()],
    'Precision (CV)': [cv_results[name]['precision']['mean'] for name in cv_results.keys()],
    'Recall (CV)': [cv_results[name]['recall']['mean'] for name in cv_results.keys()],
    'F1-Score (CV)': [cv_results[name]['f1']['mean'] for name in cv_results.keys()],
    'ROC-AUC (CV)': [cv_results[name]['roc_auc']['mean'] for name in cv_results.keys()]
})

# Arredondar para 4 casas decimais
cv_comparison_display = cv_comparison.copy()
for col in ['Accuracy (CV)', 'Precision (CV)', 'Recall (CV)', 'F1-Score (CV)', 'ROC-AUC (CV)']:
    cv_comparison_display[col] = cv_comparison_display[col].apply(lambda x: f"{x:.4f}")

print(cv_comparison_display.to_string(index=False))

# Identificar melhor modelo (baseado em ROC-AUC)
best_cv_idx = cv_comparison['ROC-AUC (CV)'].idxmax()
best_cv_model = cv_comparison.loc[best_cv_idx, 'Modelo']
best_cv_score = cv_comparison.loc[best_cv_idx, 'ROC-AUC (CV)']

print("\n" + "-" * 80)
print(f"üèÜ MELHOR MODELO (Cross-Validation): {best_cv_model}")
print(f"   ROC-AUC (CV): {best_cv_score:.4f}")
print("=" * 80)


# ============================================================================
# C√âLULA DE C√ìDIGO - Visualiza√ß√µes
# ============================================================================

# Gr√°fico de Boxplots para cada m√©trica
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

for idx, metric in enumerate(metrics_to_plot):
    row = idx // 3
    col = idx % 3
    ax = axes[row, col]

    # Preparar dados para boxplot
    data_to_plot = [cv_results[model_name][metric]['scores']
                    for model_name in cv_results.keys()]

    # Criar boxplot
    bp = ax.boxplot(data_to_plot, labels=[name for name in cv_results.keys()],
                    patch_artist=True, showmeans=True)

    # Colorir boxes
    colors = ['steelblue', 'coral', 'lightgreen']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    # Destacar m√©dias
    for mean in bp['means']:
        mean.set_marker('D')
        mean.set_markerfacecolor('red')
        mean.set_markersize(8)

    ax.set_title(f'{metric.upper()} - Cross-Validation (5 Folds)',
                 fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=10)
    ax.set_xticklabels([name.split()[0] for name in cv_results.keys()],
                       rotation=45, ha='right')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim([0, 1])

# Remover subplot vazio
fig.delaxes(axes[1, 2])

plt.suptitle('Distribui√ß√£o de Scores - Cross-Validation (5 Folds)',
             fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

print("\nüìä Interpreta√ß√£o dos Boxplots:")
print("  - Caixa (box): 50% central dos scores (Q1 a Q3)")
print("  - Linha no meio: Mediana")
print("  - Diamante vermelho: M√©dia")
print("  - Linhas (whiskers): Valores m√≠nimo e m√°ximo")
print("  - Caixa mais estreita: Modelo mais est√°vel (baixa vari√¢ncia)")


# ============================================================================
# C√âLULA DE C√ìDIGO - Compara√ß√£o CV vs Teste
# ============================================================================

print("\n" + "=" * 80)
print("COMPARA√á√ÉO: CROSS-VALIDATION vs CONJUNTO DE TESTE")
print("=" * 80)

# Criar tabela comparativa
comparison_cv_vs_test = pd.DataFrame({
    'Modelo': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'ROC-AUC (CV)': [
        cv_results['Logistic Regression']['roc_auc']['mean'],
        cv_results['Random Forest']['roc_auc']['mean'],
        cv_results['XGBoost']['roc_auc']['mean']
    ],
    'ROC-AUC (Test)': [
        lr_results['metrics']['roc_auc'],
        rf_results['metrics']['roc_auc'],
        xgb_results['metrics']['roc_auc']
    ]
})

# Calcular diferen√ßa
comparison_cv_vs_test['Diferen√ßa'] = abs(
    comparison_cv_vs_test['ROC-AUC (CV)'] - comparison_cv_vs_test['ROC-AUC (Test)']
)

# Formatar para exibi√ß√£o
comparison_display = comparison_cv_vs_test.copy()
for col in ['ROC-AUC (CV)', 'ROC-AUC (Test)', 'Diferen√ßa']:
    comparison_display[col] = comparison_display[col].apply(lambda x: f"{x:.4f}")

print("\n")
print(comparison_display.to_string(index=False))

print("\n" + "-" * 80)
print("AN√ÅLISE DE CONSIST√äNCIA")
print("-" * 80)

for idx, row in comparison_cv_vs_test.iterrows():
    model_name = row['Modelo']
    diff = row['Diferen√ßa']

    if diff < 0.02:
        status = "‚úì EXCELENTE"
        msg = "Modelo muito consistente entre CV e teste"
    elif diff < 0.05:
        status = "‚úì BOM"
        msg = "Consist√™ncia adequada"
    else:
        status = "‚ö†Ô∏è  ATEN√á√ÉO"
        msg = "Diferen√ßa significativa - poss√≠vel overfitting ou underfitting"

    print(f"\n{model_name}:")
    print(f"  Diferen√ßa: {diff:.4f} ‚Üí {status}")
    print(f"  {msg}")

print("\n" + "=" * 80)


# ============================================================================
# C√âLULA DE C√ìDIGO - Gr√°fico de Linha Comparativo
# ============================================================================

# Gr√°fico comparando CV vs Test
fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(len(comparison_cv_vs_test))
width = 0.35

bars1 = ax.bar(x - width/2, comparison_cv_vs_test['ROC-AUC (CV)'],
               width, label='Cross-Validation (5-Fold)',
               color='steelblue', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width/2, comparison_cv_vs_test['ROC-AUC (Test)'],
               width, label='Conjunto de Teste',
               color='coral', alpha=0.8, edgecolor='black')

# Adicionar valores nas barras
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom', fontsize=9)

ax.set_xlabel('Modelo', fontsize=12, fontweight='bold')
ax.set_ylabel('ROC-AUC Score', fontsize=12, fontweight='bold')
ax.set_title('Compara√ß√£o: Cross-Validation vs Conjunto de Teste',
             fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(comparison_cv_vs_test['Modelo'], rotation=45, ha='right')
ax.legend(fontsize=10)
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0, 1])

plt.tight_layout()
plt.show()


# ============================================================================
# C√âLULA MARKDOWN - Conclus√£o
# ============================================================================
"""
### üìä Conclus√£o da Valida√ß√£o Cruzada

**Por que Cross-Validation √© importante?**

1. **Confiabilidade:** Resultados de CV s√£o mais confi√°veis que um √∫nico split treino/teste
2. **Generaliza√ß√£o:** Se CV e Teste s√£o similares, o modelo generaliza bem
3. **Robustez:** Baixa vari√¢ncia entre folds indica modelo est√°vel
4. **Boas Pr√°ticas:** Requerido para publica√ß√µes cient√≠ficas e trabalhos acad√™micos

**O que observar:**

‚úÖ **CV ‚âà Test:** Modelo consistente e confi√°vel
‚úÖ **Baixa vari√¢ncia:** Modelo est√°vel (desvio padr√£o < 0.05)
‚úÖ **CV > Test:** Normal (CV usa menos dados por fold)
‚ö†Ô∏è **CV << Test:** Poss√≠vel overfitting no conjunto de teste
‚ö†Ô∏è **Alta vari√¢ncia:** Modelo inst√°vel ou dados heterog√™neos

**Valida√ß√£o Cruzada confirma que nossos modelos s√£o robustos e confi√°veis!**
"""

print("\n‚úì Se√ß√£o de Cross-Validation conclu√≠da!")
print("  Os modelos foram validados com 5-fold stratified cross-validation")
print("  Resultados confirmam robustez e generaliza√ß√£o adequada\n")
